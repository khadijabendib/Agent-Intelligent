{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWi/Xrbvm1jNR5Dm1TeB5C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khadijabendib/Agent-Intelligent/blob/main/Robotique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La création de l'environnement avec des murs et la position cible "
      ],
      "metadata": {
        "id": "vcoMnseyQi4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Définir la taille de l'environnement\n",
        "n_rows = 20\n",
        "n_cols = 20\n",
        "\n",
        "matrice = np.zeros((n_rows, n_cols), dtype=int)\n",
        "\n",
        "# Ajouter les murs\n",
        "matrice[0, :] = -1\n",
        "matrice[-1, :] = -1\n",
        "matrice[:, 0] = -1\n",
        "matrice[:,9] = -1\n",
        "matrice[9,:] = -1\n",
        "matrice[9,7] = 0\n",
        "matrice[9,16] = 0\n",
        "matrice[5,9] = 0\n",
        "matrice[16,9] = 0\n",
        "matrice[:, -1] = -1\n",
        "\n",
        "# Définir la position attendue\n",
        "goal_pos = (15, 17)\n",
        "matrice[goal_pos] = 10\n",
        "print(matrice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrL3sFhhWEGu",
        "outputId": "c64cf937-d0f0-401c-db39-f028bf51999d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0 10  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette matrice est utilisée pour stocker les valeurs Q pour chaque état-action de l'environnement."
      ],
      "metadata": {
        "id": "V3mlebGURrEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_values = np.zeros((n_rows, n_cols, 4))"
      ],
      "metadata": {
        "id": "OO7bJJ3DGrke"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La liste actions contient les différentes actions possibles dans un environnement:\n",
        "#'up' : action de déplacement vers le haut.\n",
        "#'right' : action de déplacement vers la droite.\n",
        "#'down' : action de déplacement vers le bas.\n",
        "#'left' : action de déplacement vers la gauche."
      ],
      "metadata": {
        "id": "N9X6rAJfR5Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actions=['up','right','down','left']"
      ],
      "metadata": {
        "id": "ApWz6VkhXxgv"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce code définit une fonction appelée \"is_terminal_state\" qui prend en argument les indices de la ligne et de la colonne d'une certaine position. La fonction vérifie si l'état correspondant à cette position est un état terminal.\n",
        "\n"
      ],
      "metadata": {
        "id": "IbPPqo6nd6DQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function that determines if the specified location is a terminal state\n",
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "  #if the reward for this location is 0\n",
        "  if matrice[current_row_index, current_column_index] == 0 :\n",
        "    return False\n",
        "  else:\n",
        "    return True"
      ],
      "metadata": {
        "id": "ol4NlJbdWjUD"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le code présent contient une fonction appelée \"get_starting_location\" qui joue un rôle essentiel dans la sélection d'une position de départ pour une partie ou une simulation. Cette fonction utilise la méthode de sélection aléatoire pour choisir une position de départ dans une matrice ou un environnement. Elle commence par générer des indices de ligne et de colonne aléatoires en utilisant la fonction \"np.random.randint\" en fonction des dimensions de la matrice ou de l'environnement. Ensuite, la fonction vérifie si la position générée aléatoirement est un état terminal en appelant la fonction \"is_terminal_state\". Si la position est un état terminal, la fonction génère de nouveaux indices aléatoires jusqu'à ce qu'une position non terminale soit trouvée. Une fois qu'une position non terminale est obtenue, les indices de ligne et de colonne correspondants sont renvoyés en tant que tuple, prêts à être utilisés comme position de départ valide pour le jeu ou la simulation. En intégrant cette fonction, l'application garantit que chaque partie ou simulation commence dans une position non terminale, offrant ainsi une expérience de jeu cohérente et évitant les situations indésirables où le jeu se termine immédiatement."
      ],
      "metadata": {
        "id": "D0qrENQEhKvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function that will choose a random, non-terminal starting location\n",
        "def get_starting_location():\n",
        "  #get a random row and column index\n",
        "  current_row_index = np.random.randint(n_rows)\n",
        "  current_column_index = np.random.randint(n_cols)\n",
        "\n",
        "  while is_terminal_state(current_row_index, current_column_index):\n",
        "    current_row_index = np.random.randint(n_rows)\n",
        "    current_column_index = np.random.randint(n_cols)\n",
        "  return current_row_index, current_column_index"
      ],
      "metadata": {
        "id": "2PRb5ufuWjyM"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction \"get_next_action\" joue un rôle essentiel dans le processus de sélection de la prochaine action à prendre dans un environnement d'apprentissage par renforcement. Cette fonction utilise une stratégie d'exploration-exploitation pour équilibrer l'exploration de nouvelles actions et l'exploitation des connaissances déjà acquises. Elle prend en compte le paramètre epsilon qui détermine le niveau d'exploration. Si une valeur aléatoire générée entre 0 et 1 est inférieure à epsilon, la fonction choisit la meilleure action en se basant sur les valeurs Q présentes dans la table \"q_values\" pour l'état actuel. Cela est réalisé en utilisant la fonction \"np.argmax\" pour sélectionner l'indice de l'action ayant la valeur Q maximale. En revanche, si la valeur aléatoire est supérieure ou égale à epsilon, la fonction choisit une action aléatoire parmi les actions possibles. L'intégration de cette fonction dans l'algorithme d'apprentissage par renforcement permet d'optimiser la prise de décision en exploitant les connaissances acquises tout en explorant de nouvelles actions pour améliorer les performances globales du système."
      ],
      "metadata": {
        "id": "ZJh3MPULjgc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_action(current_row_index, current_column_index, epsilon):\n",
        "  #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
        "  #then choose the most promising value from the Q-table for this state.\n",
        "  if np.random.random() < epsilon:\n",
        "    # print(\"hhhhhh\")\n",
        "    return np.argmax(q_values[current_row_index, current_column_index])\n",
        "  else: #choose a random action\n",
        "    return np.random.randint(4)"
      ],
      "metadata": {
        "id": "6LtgcFctAyhy"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction \"get_next_location\" est essentielle dans la détermination de la prochaine position en fonction de l'action choisie dans un environnement de jeu ou une simulation. Cette fonction prend en compte les indices actuels de ligne et de colonne de la position courante, ainsi que l'indice de l'action choisie. En fonction de l'action sélectionnée, la fonction ajuste les indices de ligne et de colonne de manière appropriée pour calculer la nouvelle position.\n",
        "\n"
      ],
      "metadata": {
        "id": "BLJNqp_mm1QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "  new_row_index = current_row_index\n",
        "  new_column_index = current_column_index\n",
        "  if actions[action_index] == 'up' and current_row_index > 0:\n",
        "    new_row_index -= 1\n",
        "  elif actions[action_index] == 'right' and current_column_index < n_cols - 1:\n",
        "    new_column_index += 1\n",
        "  elif actions[action_index] == 'down' and current_row_index < n_rows - 1:\n",
        "    new_row_index += 1\n",
        "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
        "    new_column_index -= 1\n",
        "  return new_row_index, new_column_index"
      ],
      "metadata": {
        "id": "xp_WBDJlBAcl"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La fonction \"get_shortest_path\" est une composante essentielle permettant d'obtenir le chemin le plus court entre n'importe quelle position autorisée dans l'entrepôt où le robot peut se déplacer et la position de l'emballage des articles. Elle utilise des fonctions auxiliaires telles que \"is_terminal_state\", \"get_next_action\" et \"get_next_location\" pour déterminer les actions à prendre et les positions successives du chemin."
      ],
      "metadata": {
        "id": "huabYOOeoYyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a function that will get the shortest path between any location within the warehouse that \n",
        "#the robot is allowed to travel and the item packaging location.\n",
        "def get_shortest_path(start_row_index, start_column_index):\n",
        "  #return immediately if this is an invalid starting location\n",
        "  if is_terminal_state(start_row_index, start_column_index):\n",
        "    return []\n",
        "  else: #if this is a 'legal' starting location\n",
        "    current_row_index, current_column_index = start_row_index, start_column_index\n",
        "    shortest_path = []\n",
        "    shortest_path.append([current_row_index, current_column_index])\n",
        "    #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
        "    while not is_terminal_state(current_row_index, current_column_index):\n",
        "      #get the best action to take\n",
        "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
        "      #move to the next location on the path, and add the new location to the list\n",
        "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
        "      shortest_path.append([current_row_index, current_column_index])\n",
        "      print(\"[\"+ str(current_row_index)  +\", \"+ str(current_column_index) +\"]\")\n",
        "    return shortest_path"
      ],
      "metadata": {
        "id": "yE4x7uxZBAnD"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création d'une représentation d'un environnement dans une matrice afin de pouvoir effectuer des opérations et des calculs sur cet environnement. Cela peut être utilisé pour développer des algorithmes de déplacement, de planification de chemin ou d'exploration dans cet environnement simulé."
      ],
      "metadata": {
        "id": "xkJDUK5vsnjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define training parameters\n",
        "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
        "discount_factor = 0.9 #discount factor for future rewards\n",
        "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
        "\n",
        "#run through 1000 training episodes\n",
        "for episode in range(100000):\n",
        "  #get the starting location for this episode\n",
        "  row_index, column_index = get_starting_location()\n",
        "  print(episode)\n",
        "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
        "  #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
        "  while not is_terminal_state(row_index, column_index):\n",
        "    #choose which action to take (i.e., where to move next)\n",
        "    action_index = get_next_action(row_index, column_index, epsilon)\n",
        "    # print(action_index)\n",
        "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
        "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
        "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
        "    \n",
        "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
        "    reward = matrice[row_index, column_index]\n",
        "    # print(reward)\n",
        "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
        "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
        "\n",
        "    #update the Q-value for the previous state and action pair\n",
        "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
        "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
        "\n",
        "print('Training complete!')"
      ],
      "metadata": {
        "id": "JSHqKSZdM0fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(matrice)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AddGoPN45a77",
        "outputId": "031dfb2b-444b-4403-f86d-3e45f4c6a3c3"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0 10  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0 -1]\n",
            " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#display a few shortest paths\n",
        "print(get_shortest_path(16, 16)) #starting at row 16, column 16\n",
        "# print(get_shortest_path(5, 0)) #starting at row 5, column 0\n",
        "print(get_shortest_path(17, 17)) #starting at row 17, column 17"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1OQ2ChSPnpK",
        "outputId": "0be8f7b2-dfd1-422f-b33d-b4664ba0dde9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15, 16]\n",
            "[15, 17]\n",
            "[[16, 16], [15, 16], [15, 17]]\n",
            "[16, 17]\n",
            "[15, 17]\n",
            "[[17, 17], [16, 17], [15, 17]]\n"
          ]
        }
      ]
    }
  ]
}